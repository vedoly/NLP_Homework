{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6031050121_HW3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpCKO19dhHPq"
      },
      "source": [
        "# HW 3 - Neural POS Tagger\n",
        "\n",
        "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow 2. Tensorflow is a deep learning framwork developed by Google to provide an easier way to use standard layers and networks.\n",
        "\n",
        "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
        "\n",
        "- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n",
        "- Neural POS Tagging with Viterbi / Marginal CRF\n",
        "\n",
        "Pretrained word embeddding are already given for you to use (albeit, a very bad one).\n",
        "\n",
        "We also provide the code for data cleaning, preprocessing and some starter code for tensorflow 2 in this notebook but feel free to modify those parts to suit your needs. Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
        "\n",
        "### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx0YbM_8hHPt"
      },
      "source": [
        "## 1. Setup and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF3lPNNkhHPu"
      },
      "source": [
        "We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n",
        "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
        "We also create a word vector for unknown word by random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS6O5yT6feRd"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58LvFz30zumq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1cfa48e-3fc9-4c96-d441-3c4b4069525c"
      },
      "source": [
        "! wget -O resources.zip https://ucb3aeb654e60f3648f04f22eb59.dl.dropboxusercontent.com/cd/0/inline/BIaRLFjy59--thi6E4swPxzV5fuh-_XpP5dyz0UFPSGy3uyj69Lj5XNKQl-6qeuSq-incriwY47uEfJEZjZI6gIP7fRHkC3sUyHAYDaLwSzmIQ/file\n",
        "!unzip resources.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-06 18:09:34--  https://ucb3aeb654e60f3648f04f22eb59.dl.dropboxusercontent.com/cd/0/inline/BIaRLFjy59--thi6E4swPxzV5fuh-_XpP5dyz0UFPSGy3uyj69Lj5XNKQl-6qeuSq-incriwY47uEfJEZjZI6gIP7fRHkC3sUyHAYDaLwSzmIQ/file\n",
            "Resolving ucb3aeb654e60f3648f04f22eb59.dl.dropboxusercontent.com (ucb3aeb654e60f3648f04f22eb59.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to ucb3aeb654e60f3648f04f22eb59.dl.dropboxusercontent.com (ucb3aeb654e60f3648f04f22eb59.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BIckZB4S76LWIZ4y52ax7dhUnxjKzSb2HQXiibE_dyB9D_uFvJiYATat2b739O1R6G2V-4vCZWktrENpgU06YboSXKTfoT5P8fMQIzGYBFgKsNq0pdcYUj5Kq3bVSvPviYJ2ySknx7vjYgE44ySxKx9EnskR9oRrY-_DrPeNQIohdkRI59niz3i9qRN3m92B2UO-YWwRVrSikrnxWw9JmEGVZueoi3jobl0u0net5H5A6toJgweOnge9ueUjApQuXEp3GA1b_Y7bOV7TvVGK_o547GHRXmXryDHMBvpjBMEYPJVwD14O8-g3gGTB_5D2DLx3pUjJ-B4BWjBuQlzrRilc/file [following]\n",
            "--2021-02-06 18:09:34--  https://ucb3aeb654e60f3648f04f22eb59.dl.dropboxusercontent.com/cd/0/inline2/BIckZB4S76LWIZ4y52ax7dhUnxjKzSb2HQXiibE_dyB9D_uFvJiYATat2b739O1R6G2V-4vCZWktrENpgU06YboSXKTfoT5P8fMQIzGYBFgKsNq0pdcYUj5Kq3bVSvPviYJ2ySknx7vjYgE44ySxKx9EnskR9oRrY-_DrPeNQIohdkRI59niz3i9qRN3m92B2UO-YWwRVrSikrnxWw9JmEGVZueoi3jobl0u0net5H5A6toJgweOnge9ueUjApQuXEp3GA1b_Y7bOV7TvVGK_o547GHRXmXryDHMBvpjBMEYPJVwD14O8-g3gGTB_5D2DLx3pUjJ-B4BWjBuQlzrRilc/file\n",
            "Reusing existing connection to ucb3aeb654e60f3648f04f22eb59.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 151484222 (144M) [application/zip]\n",
            "Saving to: ‘resources.zip’\n",
            "\n",
            "resources.zip       100%[===================>] 144.47M   116MB/s    in 1.2s    \n",
            "\n",
            "2021-02-06 18:09:36 (116 MB/s) - ‘resources.zip’ saved [151484222/151484222]\n",
            "\n",
            "Archive:  resources.zip\n",
            "  inflating: basic_ff_embedding.pt   \n",
            "   creating: data/\n",
            " extracting: data/__init__.py        \n",
            "  inflating: data/__init__.pyc       \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: data/__pycache__/orchid_corpus.cpython-36.pyc  \n",
            "  inflating: data/orchid97.txt       \n",
            "  inflating: data/orchid_corpus.py   \n",
            "  inflating: data/orchid_corpus.pyc  \n",
            "  inflating: data/orchid_test.txt    \n",
            "  inflating: data/orchid_train.txt   \n",
            "   creating: embeddings/\n",
            " extracting: embeddings/__init__.py  \n",
            "   creating: embeddings/__pycache__/\n",
            "  inflating: embeddings/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: embeddings/__pycache__/emb_reader.cpython-36.pyc  \n",
            "  inflating: embeddings/emb_reader.py  \n",
            "  inflating: embeddings/polyglot-th.pkl  \n",
            "   creating: model/\n",
            "  inflating: model/_DS_Store         \n",
            "  inflating: model/crf_basic.model   \n",
            "  inflating: model/crf_neural.model  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2a9b92hYTg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb35c06-81d6-4ee8-fc9d-7d1b484422c0"
      },
      "source": [
        "!pip install python-crfsuite\n",
        "!pip install tensorflow-addons\n",
        "!pip install tf2crf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-crfsuite in /usr/local/lib/python3.6/dist-packages (0.9.7)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: tf2crf in /usr/local/lib/python3.6/dist-packages (0.1.29)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tf2crf) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-addons>=0.8.2 in /usr/local/lib/python3.6/dist-packages (from tf2crf) (0.8.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.12)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.19.5)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.12.1)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.4.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.12.4)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.2.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (0.3.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.3.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (1.32.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.4.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (2.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tf2crf) (3.7.4.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons>=0.8.2->tf2crf) (2.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (53.0.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.24.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (4.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (2.10)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.1.0->tf2crf) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c7upY0fYsdt"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJpCbSvJhHPv"
      },
      "source": [
        "from data.orchid_corpus import get_sentences\n",
        "import numpy as np\n",
        "import numpy.random\n",
        "import tensorflow as tf\n",
        "np.random.seed(42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4SZz56ThHP0",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0020a2-66b0-4d39-cb92-03630b335cb2"
      },
      "source": [
        "yunk_emb =np.random.randn(32)\n",
        "train_data = get_sentences('train')\n",
        "test_data = get_sentences('test')\n",
        "print(train_data[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awxn_GRIhHP3"
      },
      "source": [
        "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GS3lTZshHP4"
      },
      "source": [
        "import pickle\n",
        "fp = open('basic_ff_embedding.pt', 'rb')\n",
        "embeddings = pickle.load(fp)\n",
        "fp.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CqTNKsChHP7"
      },
      "source": [
        "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPGUNEZyhHP8"
      },
      "source": [
        "## 2. Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fMWn8qehHP9"
      },
      "source": [
        "word_to_idx ={}\n",
        "idx_to_word ={}\n",
        "label_to_idx = {}\n",
        "for sentence in train_data:\n",
        "    for word,pos in sentence:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)+1\n",
        "            idx_to_word[word_to_idx[word]] = word\n",
        "        if pos not in label_to_idx:\n",
        "            label_to_idx[pos] = len(label_to_idx)+1\n",
        "word_to_idx['UNK'] = len(word_to_idx)\n",
        "\n",
        "n_classes = len(label_to_idx.keys())+1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgvZ8v_2hHP_"
      },
      "source": [
        "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktf2KkJghHQA"
      },
      "source": [
        "def word2features(sent, i, emb):\n",
        "    word = sent[i][0]\n",
        "    if word in word_to_idx :\n",
        "        return word_to_idx[word]\n",
        "    else :\n",
        "        return word_to_idx['UNK']\n",
        "\n",
        "def sent2features(sent, emb_dict):\n",
        "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [word for (word, label) in sent]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgBw3I9ShHQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b3bf94-42d7-409d-e457-d10ea76fec72"
      },
      "source": [
        "sent2features(train_data[100], embeddings)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 29, 327,   5, 328])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O7oClK-hHQG"
      },
      "source": [
        "Next we create train and test dataset, then we use tensorflow 2 to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tJxtPtohHQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52beacb-6145-46f6-daf7-8dc45c682f28"
      },
      "source": [
        "%%time\n",
        "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
        "y_train = [sent2labels(sent) for sent in train_data]\n",
        "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
        "y_test = [sent2labels(sent) for sent in test_data]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 268 ms, sys: 0 ns, total: 268 ms\n",
            "Wall time: 269 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M03BNSHYIe75"
      },
      "source": [
        "y_test = [sent2labels(sent) for sent in test_data]\r\n",
        "x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\r\n",
        "for i in range(len(y_train)):\r\n",
        "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\r\n",
        "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG1gvJ4mhHQJ"
      },
      "source": [
        "x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "y_train=tf.keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "x_test=tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
        "y_temp =[]\n",
        "for i in range(len(y_train)):\n",
        "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
        "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
        "del(y_temp)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU9x6VdehHQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89639dc3-d2ea-4420-d33a-a9cb83a26820"
      },
      "source": [
        "print(x_train[100],x_train.shape)\n",
        "print(y_train[100][3],y_train.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] (18500, 102, 48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx_c3-LwhHQP"
      },
      "source": [
        "## 3. Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvCW24orhHQP"
      },
      "source": [
        "Our output from tf keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
        "\n",
        "evaluation_report is the same as in the demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqgp2gd4hHQQ"
      },
      "source": [
        "def outputToLabel(yt,seq_len):\n",
        "    out = []\n",
        "    for i in range(0,len(yt)):\n",
        "        if(i==seq_len):\n",
        "            break\n",
        "        out.append(np.argmax(yt[i]))\n",
        "    return out"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzIL_rsAhHQT"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def evaluation_report(y_true, y_pred):\n",
        "    # retrieve all tags in y_true\n",
        "    tag_set = set()\n",
        "    for sent in y_true:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    for sent in y_pred:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    tag_list = sorted(list(tag_set))\n",
        "    \n",
        "    # count correct points\n",
        "    tag_info = dict()\n",
        "    for tag in tag_list:\n",
        "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
        "\n",
        "    all_correct = 0\n",
        "    all_count = sum([len(sent) for sent in y_true])\n",
        "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
        "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
        "            if tag_true == tag_pred:\n",
        "                tag_info[tag_true]['correct_tagged'] += 1\n",
        "                all_correct += 1\n",
        "            tag_info[tag_true]['y_true'] += 1\n",
        "            tag_info[tag_pred]['y_pred'] += 1\n",
        "    accuracy = (all_correct / all_count) * 100\n",
        "            \n",
        "    # summarize and make evaluation result\n",
        "    eval_list = list()\n",
        "    for tag in tag_list:\n",
        "        eval_result = dict()\n",
        "        eval_result['tag'] = tag\n",
        "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
        "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
        "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
        "        eval_result['precision'] = precision\n",
        "        eval_result['recall'] = recall\n",
        "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
        "        \n",
        "        eval_list.append(eval_result)\n",
        "\n",
        "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
        "    \n",
        "    df = pd.DataFrame.from_dict(eval_list)\n",
        "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
        "    display(df)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YG-RiHdhHQV"
      },
      "source": [
        "## 4. Train a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HrkAiMFhHQW"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U75Ivn2vhHQZ"
      },
      "source": [
        "The model is this section is separated to two groups\n",
        "\n",
        "- Neural POS Tagger (4.1)\n",
        "- Neural CRF POS Tagger (4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLwL3B7rhHQZ"
      },
      "source": [
        "## 4.1.1 Neural POS Tagger  (Example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVoB-1XVhHQa"
      },
      "source": [
        "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch.\n",
        "\n",
        "Instead of using tensorflow.keras.models.Sequential, we use tensorflow.keras.models.Model. The latter is better as it can have multiple input/output, of which Sequential model could not. Due to this reason, the Model class is widely used for building a complex deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxBvv9qfhHQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3492709-6d17-4ac1-aa0e-0a8199507cc6"
      },
      "source": [
        "inputs = Input(shape=(102,), dtype='int32')\n",
        "output = (Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))(inputs)\n",
        "output = Bidirectional(GRU(32, return_sequences=True))(output)\n",
        "output = Dropout(0.2)(output)\n",
        "output = TimeDistributed(Dense(n_classes,activation='softmax'))(output)\n",
        "model = Model(inputs, output)\n",
        "model.compile(optimizer=Adam(lr=0.001),  loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "model.summary()\n",
        "model.fit(x_train,y=y_train, batch_size=64,epochs=10,verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 102)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 102, 32)           480608    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 102, 64)           12672     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 102, 64)           0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 102, 48)           3120      \n",
            "=================================================================\n",
            "Total params: 496,400\n",
            "Trainable params: 496,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "290/290 [==============================] - 18s 19ms/step - loss: 0.3935 - categorical_accuracy: 0.3655\n",
            "Epoch 2/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0714 - categorical_accuracy: 0.8836\n",
            "Epoch 3/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0385 - categorical_accuracy: 0.9320\n",
            "Epoch 4/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0292 - categorical_accuracy: 0.9442\n",
            "Epoch 5/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0257 - categorical_accuracy: 0.9494\n",
            "Epoch 6/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0229 - categorical_accuracy: 0.9545\n",
            "Epoch 7/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0212 - categorical_accuracy: 0.9556\n",
            "Epoch 8/10\n",
            "290/290 [==============================] - 5s 18ms/step - loss: 0.0196 - categorical_accuracy: 0.9587\n",
            "Epoch 9/10\n",
            "290/290 [==============================] - 5s 18ms/step - loss: 0.0189 - categorical_accuracy: 0.9599\n",
            "Epoch 10/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0185 - categorical_accuracy: 0.9604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f526da868d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo9Da8MThHQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a04a89-2132-49af-ed5e-e362a835e3f8"
      },
      "source": [
        "%%time\n",
        "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "290/290 [==============================] - 5s 18ms/step - loss: 0.0175 - categorical_accuracy: 0.9626\n",
            "Epoch 2/10\n",
            "290/290 [==============================] - 5s 18ms/step - loss: 0.0168 - categorical_accuracy: 0.9641\n",
            "Epoch 3/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0163 - categorical_accuracy: 0.9648\n",
            "Epoch 4/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0157 - categorical_accuracy: 0.9662\n",
            "Epoch 5/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0152 - categorical_accuracy: 0.9671\n",
            "Epoch 6/10\n",
            "290/290 [==============================] - 5s 18ms/step - loss: 0.0148 - categorical_accuracy: 0.9682\n",
            "Epoch 7/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0143 - categorical_accuracy: 0.9693\n",
            "Epoch 8/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0139 - categorical_accuracy: 0.9697\n",
            "Epoch 9/10\n",
            "290/290 [==============================] - 5s 18ms/step - loss: 0.0134 - categorical_accuracy: 0.9710\n",
            "Epoch 10/10\n",
            "290/290 [==============================] - 5s 19ms/step - loss: 0.0131 - categorical_accuracy: 0.9716\n",
            "CPU times: user 1min 12s, sys: 8.44 s, total: 1min 20s\n",
            "Wall time: 54 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f52868b2240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yusa214hhHQh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "62c597e7-a9a5-40b1-9693-d91a0cac5374"
      },
      "source": [
        "%%time\n",
        "#model.save_weights('/data/my_pos_no_crf.h5')\n",
        "#model.load_weights('/data/my_pos_no_crf.h5')\n",
        "y_pred=model.predict(x_test)\n",
        "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
        "evaluation_report(y_test, ypred)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>99.8638</td>\n",
              "      <td>99.4573</td>\n",
              "      <td>99.6601</td>\n",
              "      <td>3665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>94.4808</td>\n",
              "      <td>94.6411</td>\n",
              "      <td>94.5609</td>\n",
              "      <td>7806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>89.7288</td>\n",
              "      <td>96.5717</td>\n",
              "      <td>93.0246</td>\n",
              "      <td>16310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>99.9455</td>\n",
              "      <td>99.3886</td>\n",
              "      <td>99.6663</td>\n",
              "      <td>12843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>95.6522</td>\n",
              "      <td>98.5075</td>\n",
              "      <td>97.0588</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>98.954</td>\n",
              "      <td>90.613</td>\n",
              "      <td>94.6</td>\n",
              "      <td>473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>98.2091</td>\n",
              "      <td>97.595</td>\n",
              "      <td>97.9011</td>\n",
              "      <td>2029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>70.3822</td>\n",
              "      <td>53.253</td>\n",
              "      <td>60.631</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>70.7165</td>\n",
              "      <td>61.6848</td>\n",
              "      <td>65.8926</td>\n",
              "      <td>227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>61.9643</td>\n",
              "      <td>41.3588</td>\n",
              "      <td>49.6069</td>\n",
              "      <td>347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>86.3158</td>\n",
              "      <td>95.3488</td>\n",
              "      <td>90.6077</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>95.6258</td>\n",
              "      <td>98.2522</td>\n",
              "      <td>96.9212</td>\n",
              "      <td>787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>90.8419</td>\n",
              "      <td>85.5593</td>\n",
              "      <td>88.1215</td>\n",
              "      <td>3075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>95.0565</td>\n",
              "      <td>93.5325</td>\n",
              "      <td>94.2883</td>\n",
              "      <td>5134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>80.2165</td>\n",
              "      <td>72.6381</td>\n",
              "      <td>76.2395</td>\n",
              "      <td>815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>89.3737</td>\n",
              "      <td>86.1366</td>\n",
              "      <td>87.7252</td>\n",
              "      <td>2069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>97.2763</td>\n",
              "      <td>85.6164</td>\n",
              "      <td>91.0747</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>98.0359</td>\n",
              "      <td>99.3939</td>\n",
              "      <td>98.7102</td>\n",
              "      <td>1148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>97.9472</td>\n",
              "      <td>96.8116</td>\n",
              "      <td>97.3761</td>\n",
              "      <td>334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>98.2818</td>\n",
              "      <td>96.9492</td>\n",
              "      <td>97.6109</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>93.0323</td>\n",
              "      <td>95.1815</td>\n",
              "      <td>94.0946</td>\n",
              "      <td>1442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>84.3373</td>\n",
              "      <td>78.6026</td>\n",
              "      <td>81.3691</td>\n",
              "      <td>1260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>89.3788</td>\n",
              "      <td>94.8264</td>\n",
              "      <td>92.022</td>\n",
              "      <td>1338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>92.1472</td>\n",
              "      <td>82.0765</td>\n",
              "      <td>86.8208</td>\n",
              "      <td>751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>91.3194</td>\n",
              "      <td>63.6804</td>\n",
              "      <td>75.0357</td>\n",
              "      <td>263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>95.6522</td>\n",
              "      <td>87.5</td>\n",
              "      <td>91.3947</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>92.8571</td>\n",
              "      <td>79.3893</td>\n",
              "      <td>85.5967</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>29</td>\n",
              "      <td>93.75</td>\n",
              "      <td>94.9367</td>\n",
              "      <td>94.3396</td>\n",
              "      <td>300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>30</td>\n",
              "      <td>70.6422</td>\n",
              "      <td>75.4902</td>\n",
              "      <td>72.9858</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>31</td>\n",
              "      <td>50.3145</td>\n",
              "      <td>77.6699</td>\n",
              "      <td>61.0687</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32</td>\n",
              "      <td>76.5152</td>\n",
              "      <td>56.7416</td>\n",
              "      <td>65.1613</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>33</td>\n",
              "      <td>91.8919</td>\n",
              "      <td>50</td>\n",
              "      <td>64.7619</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>34</td>\n",
              "      <td>86.0738</td>\n",
              "      <td>91.2811</td>\n",
              "      <td>88.601</td>\n",
              "      <td>513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>35</td>\n",
              "      <td>50</td>\n",
              "      <td>55.5556</td>\n",
              "      <td>52.6316</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>36</td>\n",
              "      <td>100</td>\n",
              "      <td>75</td>\n",
              "      <td>85.7143</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>37</td>\n",
              "      <td>90.1786</td>\n",
              "      <td>99.0196</td>\n",
              "      <td>94.3925</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>38</td>\n",
              "      <td>53.4884</td>\n",
              "      <td>58.9744</td>\n",
              "      <td>56.0976</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>39</td>\n",
              "      <td>79.3333</td>\n",
              "      <td>85</td>\n",
              "      <td>82.069</td>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>40</td>\n",
              "      <td>100</td>\n",
              "      <td>99.6429</td>\n",
              "      <td>99.8211</td>\n",
              "      <td>279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>41</td>\n",
              "      <td>68.1818</td>\n",
              "      <td>75</td>\n",
              "      <td>71.4286</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>42</td>\n",
              "      <td>100</td>\n",
              "      <td>70.5882</td>\n",
              "      <td>82.7586</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>25</td>\n",
              "      <td>11.1111</td>\n",
              "      <td>15.3846</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>46</td>\n",
              "      <td>50</td>\n",
              "      <td>20</td>\n",
              "      <td>28.5714</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>accuracy=93.10</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag precision   recall  f_score correct_count\n",
              "0                1   99.8638  99.4573  99.6601          3665\n",
              "1                2   94.4808  94.6411  94.5609          7806\n",
              "2                3   89.7288  96.5717  93.0246         16310\n",
              "3                4   99.9455  99.3886  99.6663         12843\n",
              "4                5   95.6522  98.5075  97.0588            66\n",
              "5                6    98.954   90.613     94.6           473\n",
              "6                7   98.2091   97.595  97.9011          2029\n",
              "7                8   70.3822   53.253   60.631           221\n",
              "8                9   70.7165  61.6848  65.8926           227\n",
              "9               10   61.9643  41.3588  49.6069           347\n",
              "10              11   86.3158  95.3488  90.6077            82\n",
              "11              12   95.6258  98.2522  96.9212           787\n",
              "12              13   90.8419  85.5593  88.1215          3075\n",
              "13              14   95.0565  93.5325  94.2883          5134\n",
              "14              15   80.2165  72.6381  76.2395           815\n",
              "15              16   89.3737  86.1366  87.7252          2069\n",
              "16              17   97.2763  85.6164  91.0747           500\n",
              "17              18   98.0359  99.3939  98.7102          1148\n",
              "18              19   97.9472  96.8116  97.3761           334\n",
              "19              20   98.2818  96.9492  97.6109           286\n",
              "20              21   93.0323  95.1815  94.0946          1442\n",
              "21              22   84.3373  78.6026  81.3691          1260\n",
              "22              23   89.3788  94.8264   92.022          1338\n",
              "23              24   92.1472  82.0765  86.8208           751\n",
              "24              25   91.3194  63.6804  75.0357           263\n",
              "25              26   95.6522     87.5  91.3947           154\n",
              "26              27   92.8571  79.3893  85.5967           104\n",
              "27              29     93.75  94.9367  94.3396           300\n",
              "28              30   70.6422  75.4902  72.9858            77\n",
              "29              31   50.3145  77.6699  61.0687            80\n",
              "30              32   76.5152  56.7416  65.1613           101\n",
              "31              33   91.8919       50  64.7619            34\n",
              "32              34   86.0738  91.2811   88.601           513\n",
              "33              35        50  55.5556  52.6316             5\n",
              "34              36       100       75  85.7143            12\n",
              "35              37   90.1786  99.0196  94.3925           101\n",
              "36              38   53.4884  58.9744  56.0976            23\n",
              "37              39   79.3333       85   82.069           119\n",
              "38              40       100  99.6429  99.8211           279\n",
              "39              41   68.1818       75  71.4286            15\n",
              "40              42       100  70.5882  82.7586            12\n",
              "41              43        25  11.1111  15.3846             1\n",
              "42              45         0        0        -             0\n",
              "43              46        50       20  28.5714             1\n",
              "44  accuracy=93.10                                          "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.98 s, sys: 91.1 ms, total: 3.07 s\n",
            "Wall time: 2.82 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0NKka14hHQw"
      },
      "source": [
        "## 4.2 CRF Viterbi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijd1rwTghHQx"
      },
      "source": [
        "Your next task is to incorporate Conditional random fields (CRF) to your model.\n",
        "\n",
        "To use the CRF layer, you need to use an extension repository for tensorflow library, call tf2crf. If you want to see the detailed implementation, you should read the official tensorflow extention of CRF (https://www.tensorflow.org/addons/api_docs/python/tfa/text).\n",
        "\n",
        "tf2crf link :  https://github.com/xuxingya/tf2crf\n",
        "\n",
        "For inference, you should look at crf.py at the method call and view the input/output argmunets. \n",
        "Link : https://github.com/xuxingya/tf2crf/blob/master/tf2crf/crf.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuybajePhHQy"
      },
      "source": [
        "### 4.2.1 CRF without pretrained weight\n",
        "### #TODO 1\n",
        "Incoperate CRF layer to your model in 4.1. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
        "\n",
        "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "Do not forget to save this model weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFAO7TOtSaUT",
        "outputId": "67cfdcc3-f4e0-4cc4-d7bd-cd4a4aff6c00"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18500, 102)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ksgNw8OSctH",
        "outputId": "c6358dfc-5aa4-4f15-b69e-63d7fbf02558"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18500, 102, 48)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEV0q1vAhHQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02749785-676c-440f-a0c9-6d9ab4727f9b"
      },
      "source": [
        "import numpy\r\n",
        "from tf2crf import CRF, ModelWithCRFLoss\r\n",
        "\r\n",
        "\r\n",
        "inputs = Input(shape=(102,), dtype='int32')\r\n",
        "output = (Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))(inputs)\r\n",
        "output = Bidirectional(GRU(32, return_sequences=True))(output)\r\n",
        "output = Dropout(0.2)(output)\r\n",
        "output = Dense(n_classes)(output)\r\n",
        "crf = CRF(dtype='float32')\r\n",
        "output = crf(output)\r\n",
        "model = Model(inputs, output)\r\n",
        "model = ModelWithCRFLoss(model)\r\n",
        "model.compile(optimizer=Adam(lr=0.001), metrics=['categorical_accuracy'])\r\n",
        "\r\n",
        "# model.summary()\r\n",
        "model.fit(x_train,y=np.argmax(y_train, axis=2), batch_size=64,epochs=13,verbose=1)\r\n",
        "\r\n",
        "model.save_weights('to_do1.h5')\r\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/13\n",
            "290/290 [==============================] - 69s 212ms/step - crf_loss: 26.1031 - accuracy: 0.5658\n",
            "Epoch 2/13\n",
            "290/290 [==============================] - 62s 212ms/step - crf_loss: 5.8303 - accuracy: 0.9078\n",
            "Epoch 3/13\n",
            "290/290 [==============================] - 62s 213ms/step - crf_loss: 3.6437 - accuracy: 0.9361\n",
            "Epoch 4/13\n",
            "290/290 [==============================] - 62s 215ms/step - crf_loss: 2.9005 - accuracy: 0.9461\n",
            "Epoch 5/13\n",
            "290/290 [==============================] - 62s 212ms/step - crf_loss: 2.5621 - accuracy: 0.9509\n",
            "Epoch 6/13\n",
            "290/290 [==============================] - 61s 209ms/step - crf_loss: 2.3431 - accuracy: 0.9539\n",
            "Epoch 7/13\n",
            "290/290 [==============================] - 60s 207ms/step - crf_loss: 2.1868 - accuracy: 0.9562\n",
            "Epoch 8/13\n",
            "290/290 [==============================] - 60s 207ms/step - crf_loss: 2.0427 - accuracy: 0.9583\n",
            "Epoch 9/13\n",
            "290/290 [==============================] - 60s 208ms/step - crf_loss: 1.9442 - accuracy: 0.9600\n",
            "Epoch 10/13\n",
            "290/290 [==============================] - 60s 208ms/step - crf_loss: 1.8579 - accuracy: 0.9613\n",
            "Epoch 11/13\n",
            "290/290 [==============================] - 61s 209ms/step - crf_loss: 1.7854 - accuracy: 0.9623\n",
            "Epoch 12/13\n",
            "290/290 [==============================] - 61s 211ms/step - crf_loss: 1.7058 - accuracy: 0.9644\n",
            "Epoch 13/13\n",
            "290/290 [==============================] - 61s 211ms/step - crf_loss: 1.6237 - accuracy: 0.9656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CADuMz2dmFBo",
        "outputId": "13920ecb-be0a-45d0-c15a-8384dcce8868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model.load_weights('/data/my_pos_no_crf.h5')\r\n",
        "y_pred=model.predict(x_test)\r\n",
        "\r\n",
        "evaluation_report(y_test, y_pred[0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>99.7827</td>\n",
              "      <td>99.7015</td>\n",
              "      <td>99.7421</td>\n",
              "      <td>3674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>95.1404</td>\n",
              "      <td>94.4714</td>\n",
              "      <td>94.8047</td>\n",
              "      <td>7792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>91.1425</td>\n",
              "      <td>96.2638</td>\n",
              "      <td>93.6332</td>\n",
              "      <td>16258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>99.9922</td>\n",
              "      <td>99.644</td>\n",
              "      <td>99.8178</td>\n",
              "      <td>12876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>88</td>\n",
              "      <td>98.5075</td>\n",
              "      <td>92.9577</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>99.7817</td>\n",
              "      <td>87.5479</td>\n",
              "      <td>93.2653</td>\n",
              "      <td>457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>97.5551</td>\n",
              "      <td>97.8836</td>\n",
              "      <td>97.7191</td>\n",
              "      <td>2035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>69.2833</td>\n",
              "      <td>48.9157</td>\n",
              "      <td>57.3446</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>61.0667</td>\n",
              "      <td>62.2283</td>\n",
              "      <td>61.642</td>\n",
              "      <td>229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>62.1053</td>\n",
              "      <td>42.1931</td>\n",
              "      <td>50.2484</td>\n",
              "      <td>354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>88.8889</td>\n",
              "      <td>93.0233</td>\n",
              "      <td>90.9091</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>96.2195</td>\n",
              "      <td>98.5019</td>\n",
              "      <td>97.3473</td>\n",
              "      <td>789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>89.3666</td>\n",
              "      <td>85.5871</td>\n",
              "      <td>87.436</td>\n",
              "      <td>3076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>94.5584</td>\n",
              "      <td>94.0244</td>\n",
              "      <td>94.2907</td>\n",
              "      <td>5161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>83.4719</td>\n",
              "      <td>71.5686</td>\n",
              "      <td>77.0633</td>\n",
              "      <td>803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>88.5033</td>\n",
              "      <td>88.1349</td>\n",
              "      <td>88.3187</td>\n",
              "      <td>2117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>97.5855</td>\n",
              "      <td>83.0479</td>\n",
              "      <td>89.7317</td>\n",
              "      <td>485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>98.6219</td>\n",
              "      <td>99.1342</td>\n",
              "      <td>98.8774</td>\n",
              "      <td>1145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>97.6401</td>\n",
              "      <td>95.942</td>\n",
              "      <td>96.7836</td>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>98.2818</td>\n",
              "      <td>96.9492</td>\n",
              "      <td>97.6109</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>93.5484</td>\n",
              "      <td>93.7954</td>\n",
              "      <td>93.6717</td>\n",
              "      <td>1421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>82.3752</td>\n",
              "      <td>81.3475</td>\n",
              "      <td>81.8581</td>\n",
              "      <td>1304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>89.3255</td>\n",
              "      <td>96.669</td>\n",
              "      <td>92.8523</td>\n",
              "      <td>1364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>86.755</td>\n",
              "      <td>85.9016</td>\n",
              "      <td>86.3262</td>\n",
              "      <td>786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>90.4437</td>\n",
              "      <td>64.1646</td>\n",
              "      <td>75.0708</td>\n",
              "      <td>265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>93.7853</td>\n",
              "      <td>94.3182</td>\n",
              "      <td>94.051</td>\n",
              "      <td>166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>92.5</td>\n",
              "      <td>84.7328</td>\n",
              "      <td>88.4462</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>93.6556</td>\n",
              "      <td>98.1013</td>\n",
              "      <td>95.8269</td>\n",
              "      <td>310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>71.2963</td>\n",
              "      <td>75.4902</td>\n",
              "      <td>73.3333</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>47.191</td>\n",
              "      <td>81.5534</td>\n",
              "      <td>59.7865</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>71.1409</td>\n",
              "      <td>59.5506</td>\n",
              "      <td>64.8318</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>85.3659</td>\n",
              "      <td>51.4706</td>\n",
              "      <td>64.2202</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>88.5764</td>\n",
              "      <td>89.6797</td>\n",
              "      <td>89.1247</td>\n",
              "      <td>504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>83.3333</td>\n",
              "      <td>55.5556</td>\n",
              "      <td>66.6667</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>93.5185</td>\n",
              "      <td>99.0196</td>\n",
              "      <td>96.1905</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>62.5</td>\n",
              "      <td>51.2821</td>\n",
              "      <td>56.338</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>77.6398</td>\n",
              "      <td>89.2857</td>\n",
              "      <td>83.0565</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>88.8889</td>\n",
              "      <td>80</td>\n",
              "      <td>84.2105</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>100</td>\n",
              "      <td>88.2353</td>\n",
              "      <td>93.75</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>accuracy=93.28</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag precision   recall  f_score correct_count\n",
              "0                0         -        0        -             0\n",
              "1                1   99.7827  99.7015  99.7421          3674\n",
              "2                2   95.1404  94.4714  94.8047          7792\n",
              "3                3   91.1425  96.2638  93.6332         16258\n",
              "4                4   99.9922   99.644  99.8178         12876\n",
              "5                5        88  98.5075  92.9577            66\n",
              "6                6   99.7817  87.5479  93.2653           457\n",
              "7                7   97.5551  97.8836  97.7191          2035\n",
              "8                8   69.2833  48.9157  57.3446           203\n",
              "9                9   61.0667  62.2283   61.642           229\n",
              "10              10   62.1053  42.1931  50.2484           354\n",
              "11              11   88.8889  93.0233  90.9091            80\n",
              "12              12   96.2195  98.5019  97.3473           789\n",
              "13              13   89.3666  85.5871   87.436          3076\n",
              "14              14   94.5584  94.0244  94.2907          5161\n",
              "15              15   83.4719  71.5686  77.0633           803\n",
              "16              16   88.5033  88.1349  88.3187          2117\n",
              "17              17   97.5855  83.0479  89.7317           485\n",
              "18              18   98.6219  99.1342  98.8774          1145\n",
              "19              19   97.6401   95.942  96.7836           331\n",
              "20              20   98.2818  96.9492  97.6109           286\n",
              "21              21   93.5484  93.7954  93.6717          1421\n",
              "22              22   82.3752  81.3475  81.8581          1304\n",
              "23              23   89.3255   96.669  92.8523          1364\n",
              "24              24    86.755  85.9016  86.3262           786\n",
              "25              25   90.4437  64.1646  75.0708           265\n",
              "26              26   93.7853  94.3182   94.051           166\n",
              "27              27      92.5  84.7328  88.4462           111\n",
              "28              29   93.6556  98.1013  95.8269           310\n",
              "29              30   71.2963  75.4902  73.3333            77\n",
              "30              31    47.191  81.5534  59.7865            84\n",
              "31              32   71.1409  59.5506  64.8318           106\n",
              "32              33   85.3659  51.4706  64.2202            35\n",
              "33              34   88.5764  89.6797  89.1247           504\n",
              "34              35   83.3333  55.5556  66.6667             5\n",
              "35              36       100      100      100            16\n",
              "36              37   93.5185  99.0196  96.1905           101\n",
              "37              38      62.5  51.2821   56.338            20\n",
              "38              39   77.6398  89.2857  83.0565           125\n",
              "39              40       100      100      100           280\n",
              "40              41   88.8889       80  84.2105            16\n",
              "41              42       100  88.2353    93.75            15\n",
              "42              43         0        0        -             0\n",
              "43              45         0        0        -             0\n",
              "44              46         -        0        -             0\n",
              "45  accuracy=93.28                                          "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOnHO-__kj7T"
      },
      "source": [
        "\n",
        "### 4.2.2 CRF with pretrained weight\n",
        "\n",
        "### #TODO 2\n",
        "\n",
        "We would like you create a neural CRF POS tagger model  with the pretrained word embedding as an input and the word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus.\n",
        "\n",
        "Optionally, you can use your own pretrained word embedding.\n",
        "\n",
        "#### Hint: You can get the embedding from get_embeddings function from embeddings/emb_reader.py . \n",
        "\n",
        "(You may want to read about Tensorflow Masking layer and Trainable parameter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhZI1RoPe2xL"
      },
      "source": [
        "import embeddings.emb_reader as emb"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCy4-vmne8Tj"
      },
      "source": [
        "pretrained_dict =  emb.get_embeddings()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h49eX7iJmyAF",
        "outputId": "a6eba06e-7e73-4d0b-da06-8b2445757814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding_matrix = []\r\n",
        "count = 0\r\n",
        "for key in word_to_idx:\r\n",
        "  if key in pretrained_dict:\r\n",
        "    embedding_matrix.append(pretrained_dict[key])\r\n",
        "  else:\r\n",
        "    count += 1\r\n",
        "    embedding_matrix.append(np.zeros(64))\r\n",
        "print(count)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5uqLehZnwt1",
        "outputId": "88aca4d4-e15c-4a3b-e9b4-3ba26d5237d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array(embedding_matrix).shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15019, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVBAv3a9kanH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c787227-8a3d-43c8-ec54-e7c5bf962631"
      },
      "source": [
        "import numpy\r\n",
        "from tf2crf import CRF, ModelWithCRFLoss\r\n",
        "from keras import initializers\r\n",
        "\r\n",
        "inputs = Input(shape=(102,), dtype='int32')\r\n",
        "output = (Embedding(len(word_to_idx),64,input_length=102,mask_zero=True,embeddings_initializer=initializers.Constant(embedding_matrix)))(inputs)\r\n",
        "output = Bidirectional(GRU(64, return_sequences=True))(output)\r\n",
        "output = Dropout(0.2)(output)\r\n",
        "crf = CRF(dtype='float32')\r\n",
        "output = crf(output)\r\n",
        "model = Model(inputs, output)\r\n",
        "model = ModelWithCRFLoss(model)\r\n",
        "model.compile(optimizer=Adam(lr=0.001), metrics=['categorical_accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train,y=np.argmax(y_train, axis=2), batch_size=1024,epochs=13,verbose=1)\r\n",
        "model.summary()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/13\n",
            "290/290 [==============================] - 73s 227ms/step - crf_loss: 51.6465 - accuracy: 0.2233\n",
            "Epoch 2/13\n",
            "290/290 [==============================] - 65s 226ms/step - crf_loss: 38.7382 - accuracy: 0.3560\n",
            "Epoch 3/13\n",
            "290/290 [==============================] - 65s 225ms/step - crf_loss: 31.3212 - accuracy: 0.4590\n",
            "Epoch 4/13\n",
            "290/290 [==============================] - 66s 226ms/step - crf_loss: 26.1557 - accuracy: 0.5974\n",
            "Epoch 5/13\n",
            "290/290 [==============================] - 66s 227ms/step - crf_loss: 22.5115 - accuracy: 0.6728\n",
            "Epoch 6/13\n",
            "290/290 [==============================] - 66s 226ms/step - crf_loss: 18.7231 - accuracy: 0.8211\n",
            "Epoch 7/13\n",
            "290/290 [==============================] - 66s 227ms/step - crf_loss: 16.4863 - accuracy: 0.8426\n",
            "Epoch 8/13\n",
            "290/290 [==============================] - 65s 225ms/step - crf_loss: 15.2270 - accuracy: 0.8428\n",
            "Epoch 9/13\n",
            "290/290 [==============================] - 65s 225ms/step - crf_loss: 14.5125 - accuracy: 0.8352\n",
            "Epoch 10/13\n",
            "290/290 [==============================] - 66s 228ms/step - crf_loss: 14.0505 - accuracy: 0.8280\n",
            "Epoch 11/13\n",
            "290/290 [==============================] - 65s 226ms/step - crf_loss: 13.7313 - accuracy: 0.8222\n",
            "Epoch 12/13\n",
            "290/290 [==============================] - 65s 225ms/step - crf_loss: 13.4664 - accuracy: 0.8189\n",
            "Epoch 13/13\n",
            "290/290 [==============================] - 65s 225ms/step - crf_loss: 13.3528 - accuracy: 0.8175\n",
            "Model: \"model_with_crf_loss_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "model_5 (Functional)         ((None, 102), (None, 102, 1027522   \n",
            "=================================================================\n",
            "Total params: 1,027,524\n",
            "Trainable params: 1,027,520\n",
            "Non-trainable params: 4\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4j--KztuFUz",
        "outputId": "56b83e94-5ea4-4f5d-8034-27b142e4101f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_pred=model.predict(x_test)\r\n",
        "\r\n",
        "evaluation_report(y_test, y_pred[0])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>84.8258</td>\n",
              "      <td>98.4532</td>\n",
              "      <td>91.1329</td>\n",
              "      <td>3628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>91.0856</td>\n",
              "      <td>91.5495</td>\n",
              "      <td>91.317</td>\n",
              "      <td>7551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>80.7324</td>\n",
              "      <td>97.2526</td>\n",
              "      <td>88.2258</td>\n",
              "      <td>16425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>92.6866</td>\n",
              "      <td>98.8624</td>\n",
              "      <td>95.675</td>\n",
              "      <td>12775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>99.7696</td>\n",
              "      <td>82.9502</td>\n",
              "      <td>90.5858</td>\n",
              "      <td>433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>96.7376</td>\n",
              "      <td>65.6085</td>\n",
              "      <td>78.1886</td>\n",
              "      <td>1364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>78.2946</td>\n",
              "      <td>24.3373</td>\n",
              "      <td>37.1324</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>62.2642</td>\n",
              "      <td>8.96739</td>\n",
              "      <td>15.677</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>78.4091</td>\n",
              "      <td>32.8963</td>\n",
              "      <td>46.3476</td>\n",
              "      <td>276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>100</td>\n",
              "      <td>36.0465</td>\n",
              "      <td>52.9915</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>91.9308</td>\n",
              "      <td>79.6504</td>\n",
              "      <td>85.3512</td>\n",
              "      <td>638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>88.4704</td>\n",
              "      <td>75.153</td>\n",
              "      <td>81.2697</td>\n",
              "      <td>2701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>84.2624</td>\n",
              "      <td>95.0082</td>\n",
              "      <td>89.3132</td>\n",
              "      <td>5215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>73.3967</td>\n",
              "      <td>55.0802</td>\n",
              "      <td>62.9328</td>\n",
              "      <td>618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>85.0412</td>\n",
              "      <td>77.3938</td>\n",
              "      <td>81.0375</td>\n",
              "      <td>1859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>94.7867</td>\n",
              "      <td>68.4932</td>\n",
              "      <td>79.5229</td>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>86.7518</td>\n",
              "      <td>94.1126</td>\n",
              "      <td>90.2824</td>\n",
              "      <td>1087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>99.4949</td>\n",
              "      <td>57.1014</td>\n",
              "      <td>72.5599</td>\n",
              "      <td>197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>95.5556</td>\n",
              "      <td>14.5763</td>\n",
              "      <td>25.2941</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>81.3526</td>\n",
              "      <td>81.7822</td>\n",
              "      <td>81.5668</td>\n",
              "      <td>1239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>78.326</td>\n",
              "      <td>55.4585</td>\n",
              "      <td>64.9379</td>\n",
              "      <td>889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>87.5298</td>\n",
              "      <td>78.1006</td>\n",
              "      <td>82.5468</td>\n",
              "      <td>1102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>72.4868</td>\n",
              "      <td>74.8634</td>\n",
              "      <td>73.6559</td>\n",
              "      <td>685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>88.3495</td>\n",
              "      <td>22.0339</td>\n",
              "      <td>35.2713</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>92.8571</td>\n",
              "      <td>44.3182</td>\n",
              "      <td>60</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>100</td>\n",
              "      <td>8.39695</td>\n",
              "      <td>15.493</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>95.3846</td>\n",
              "      <td>78.481</td>\n",
              "      <td>86.1111</td>\n",
              "      <td>248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>20</td>\n",
              "      <td>1.96078</td>\n",
              "      <td>3.57143</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>84.6154</td>\n",
              "      <td>10.6796</td>\n",
              "      <td>18.9655</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>100</td>\n",
              "      <td>4.41176</td>\n",
              "      <td>8.4507</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>88.8496</td>\n",
              "      <td>89.3238</td>\n",
              "      <td>89.0861</td>\n",
              "      <td>502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>66.6667</td>\n",
              "      <td>9.80392</td>\n",
              "      <td>17.094</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>100</td>\n",
              "      <td>2.5641</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>100</td>\n",
              "      <td>44.6429</td>\n",
              "      <td>61.7284</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>45</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>accuracy=86.20</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               tag precision   recall  f_score correct_count\n",
              "0                0         -        0        -             0\n",
              "1                1   84.8258  98.4532  91.1329          3628\n",
              "2                2   91.0856  91.5495   91.317          7551\n",
              "3                3   80.7324  97.2526  88.2258         16425\n",
              "4                4   92.6866  98.8624   95.675         12775\n",
              "5                5         -        0        -             0\n",
              "6                6   99.7696  82.9502  90.5858           433\n",
              "7                7   96.7376  65.6085  78.1886          1364\n",
              "8                8   78.2946  24.3373  37.1324           101\n",
              "9                9   62.2642  8.96739   15.677            33\n",
              "10              10   78.4091  32.8963  46.3476           276\n",
              "11              11       100  36.0465  52.9915            31\n",
              "12              12   91.9308  79.6504  85.3512           638\n",
              "13              13   88.4704   75.153  81.2697          2701\n",
              "14              14   84.2624  95.0082  89.3132          5215\n",
              "15              15   73.3967  55.0802  62.9328           618\n",
              "16              16   85.0412  77.3938  81.0375          1859\n",
              "17              17   94.7867  68.4932  79.5229           400\n",
              "18              18   86.7518  94.1126  90.2824          1087\n",
              "19              19   99.4949  57.1014  72.5599           197\n",
              "20              20   95.5556  14.5763  25.2941            43\n",
              "21              21   81.3526  81.7822  81.5668          1239\n",
              "22              22    78.326  55.4585  64.9379           889\n",
              "23              23   87.5298  78.1006  82.5468          1102\n",
              "24              24   72.4868  74.8634  73.6559           685\n",
              "25              25   88.3495  22.0339  35.2713            91\n",
              "26              26   92.8571  44.3182       60            78\n",
              "27              27       100  8.39695   15.493            11\n",
              "28              29   95.3846   78.481  86.1111           248\n",
              "29              30        20  1.96078  3.57143             2\n",
              "30              31   84.6154  10.6796  18.9655            11\n",
              "31              32         -        0        -             0\n",
              "32              33       100  4.41176   8.4507             3\n",
              "33              34   88.8496  89.3238  89.0861           502\n",
              "34              35         -        0        -             0\n",
              "35              36         -        0        -             0\n",
              "36              37   66.6667  9.80392   17.094            10\n",
              "37              38       100   2.5641        5             1\n",
              "38              39         -        0        -             0\n",
              "39              40       100  44.6429  61.7284           125\n",
              "40              41         -        0        -             0\n",
              "41              42         -        0        -             0\n",
              "42              43         -        0        -             0\n",
              "43              45         -        0        -             0\n",
              "44              46         -        0        -             0\n",
              "45  accuracy=86.20                                          "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H26CGf2shDH"
      },
      "source": [
        "import keras.backend as K\r\n",
        "K.clear_session()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVTgYKsGr763"
      },
      "source": [
        "model.save_weights('to_do2.h5')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYX749sbkzA-"
      },
      "source": [
        "### #TODO 3\n",
        "Compare the result between all neural tagger models in 4.1 and 4.2.x and provide a convincing reason and example for the result of these models (which model perform better, why?)\n",
        "\n",
        "(If you use your own weight please state so in the answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJKtOsoRhHQv"
      },
      "source": [
        "  from evalution results, 4.2.x perform better bacause CRF have a transition matrix that will put probablitity aporches to model and that will make language more sensible in the way of its principle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z8raWOawjxp"
      },
      "source": [
        "### #TODO 4\n",
        "\n",
        "Upon inference, the model also returns its transition matrix, which is learned during training. Your task is to observe and report whether the returned matrix is sensible. You can provide some examples to support your argument.\n",
        "\n",
        "#### Hint : The transition matrix must have the shape  of (num_class, num_class).\n",
        "\n",
        "<b>Write your answer here : From the  2 heat maps above (first one is crf matrix after training, second one is crf matrix before training) the Before training show that value are random but after training the heatmap change, only some of transition has high freqency while the others has less frequency. It was sensible because if we look by the view of language's principle it always has corelation about some POS with other POS but not every POS that support the assumption in first plot.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAHN4z8r3C3S"
      },
      "source": [
        "import numpy\r\n",
        "from tf2crf import CRF, ModelWithCRFLoss\r\n",
        "\r\n",
        "\r\n",
        "inputs = Input(shape=(102,), dtype='int32')\r\n",
        "output = (Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))(inputs)\r\n",
        "output = Bidirectional(GRU(32, return_sequences=True))(output)\r\n",
        "output = Dropout(0.2)(output)\r\n",
        "output = Dense(n_classes)(output)\r\n",
        "crf = CRF(dtype='float32')\r\n",
        "output = crf(output)\r\n",
        "model = Model(inputs, output)\r\n",
        "model = ModelWithCRFLoss(model)\r\n",
        "model.compile(optimizer=Adam(lr=0.001), metrics=['categorical_accuracy'])\r\n",
        "model.built = True \r\n",
        "model.load_weights('to_do1.h5')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mA8uA6TGq9W"
      },
      "source": [
        "import numpy\r\n",
        "from tf2crf import CRF, ModelWithCRFLoss\r\n",
        "\r\n",
        "\r\n",
        "inputs = Input(shape=(102,), dtype='int32')\r\n",
        "output = (Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))(inputs)\r\n",
        "output = Bidirectional(GRU(32, return_sequences=True))(output)\r\n",
        "output = Dropout(0.2)(output)\r\n",
        "output = Dense(n_classes)(output)\r\n",
        "crf_zero = CRF(dtype='float32')\r\n",
        "output = crf_zero(output)\r\n",
        "model2 = Model(inputs, output)\r\n",
        "model2 = ModelWithCRFLoss(model2)\r\n",
        "model2.compile(optimizer=Adam(lr=0.001), metrics=['categorical_accuracy'])\r\n",
        "model.built = True "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7TlVsuR5LOR",
        "outputId": "25a82828-5ee9-4e00-c38f-4c73fcc53812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "## after train\r\n",
        "\r\n",
        "import numpy as np \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "H = crf.transitions \r\n",
        "plt.imshow(H, cmap='gray')\r\n",
        "plt.show()"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmd4Ij2F8KUx",
        "outputId": "51948855-f772-4847-d7ce-017eed5640da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "## before train\r\n",
        "\r\n",
        "import numpy as np \r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "H = crf_zero.transitions \r\n",
        "plt.imshow(H, cmap = 'gray')\r\n",
        "plt.show()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJEEheQ6M7kp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsGYy7YZPHMW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}